{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SgRNA Activity Scoring\n",
    "\n",
    "After the database had been created and all sgRNAs identified, I scored the sgRNAs using published sgRNA scoring methods. \n",
    "\n",
    "## Doench Score\n",
    "\n",
    "The first method I used was the on-target activity scoring method developed by Doench et al. in <a href=\"http://www.nature.com/nbt/journal/v32/n12/full/nbt.3026.html\">this paper</a>. The calc_doench_score function was downloaded as a python script from the Broad Institute's <a href=\"http://www.broadinstitute.org/rnai/public/analysis-tools/sgrna-design-v1\">sgRNA Designer</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "### Doench Score ###\n",
    "\n",
    "\"\"\"\n",
    "Calculates the on-target score for an sgRNA\n",
    "Input: 30mer\n",
    "Output: On-target score\n",
    "Run as: python on_target_score_calculator.py <30mer>\n",
    "\"\"\"\n",
    "def calc_doench_score(s):\n",
    "    s_list = list(s)\n",
    "    s_20mer = s[4:24]\n",
    "    nuc_hash = {'A':0, 'T':1, 'C':2, 'G':3}\n",
    "    score = 0.597636154\n",
    "    gc = s_20mer.count('G')+s_20mer.count('C')\n",
    "    gc_low = -0.202625894\n",
    "    gc_high = -0.166587752\n",
    "    if gc < 10:\n",
    "        gc_val = abs(gc-10)\n",
    "        score = score+(gc_val*gc_low)\n",
    "    elif gc > 10:\n",
    "        gc_val = gc-10\n",
    "        score = score+(gc_val*gc_high)\n",
    "    #rows[1-30]cols['ATCG']\n",
    "    sing_nuc_hash = {'G2':-0.275377128,'A3':-0.323887456,'C3':0.172128871,'C4':-0.100666209,'C5':-0.20180294, \\\n",
    "                    'G5':0.245956633,'A6':0.036440041,'C6':0.098376835,'C7':-0.741181291,\\\n",
    "                    'G7':-0.393264397,'A12':-0.466099015,'A15':0.085376945,'C15':-0.013813972,\\\n",
    "                    'A16':0.272620512,'C16':-0.119022648,'T16':-0.285944222,'A17':0.097454592,\\\n",
    "                    'G17':-0.17554617,'C18':-0.345795451,'G18':-0.678096426,'A19':0.22508903,\\\n",
    "                    'C19':-0.507794051,'G20':-0.417373597,'T20':-0.054306959,'G21':0.379899366,\\\n",
    "                    'T21':-0.090712644,'C22':0.057823319,'T22':-0.530567296,'T23':-0.877007428,\\\n",
    "                    'C24':-0.876235846,'G24':0.278916259,'T24':-0.403102218,'A25':-0.077300704,\\\n",
    "                    'C25':0.287935617,'T25':-0.221637217,'G28':-0.689016682,'T28':0.117877577,\\\n",
    "                    'C29':-0.160445304,'G30':0.386342585}\n",
    "    #score_mat = np.matrix('0 0 0 0;0 0 0 -0.275377128;-0.323887456 0 0.172128871 0;0 0 -0.100666209 0;\n",
    "    #0 0 -0.20180294 0.245956633;0.036440041 0 0.098376835 0;0 0 -0.741181291 -0.393264397;0 0 0 0;0 0 0 0;\n",
    "    #0 0 0 0;0 0 0 0;-0.466099015 0 0 0;0 0 0 0;0 0 0 0;0.085376945 0 -0.013813972 0;\n",
    "    #0.272620512 -0.285944222 -0.119022648 0;0.097454592 0 0 -0.17554617;0 0 -0.345795451 -0.678096426;\n",
    "    #0.22508903 0 -0.507794051 0;0 -0.054306959 0 -0.417373597;0 -0.090712644 0 0.379899366;\n",
    "    #0 -0.530567296 0.057823319 0;0 -0.877007428 0 0;0 -0.403102218 -0.876235846 0.278916259;\n",
    "    #-0.077300704 -0.221637217 0.287935617 0;0 0 0 0;0 0 0 0;0 0.117877577 0 -0.689016682;\n",
    "    #0 0 -0.160445304 0;0 0 0 0.386342585')\n",
    "    dinuc_hash = {'GT2':-0.625778696,'GC5':0.300043317,'AA6':-0.834836245,'TA6':0.760627772,\n",
    "                  'GG7':-0.490816749,'GG12':-1.516907439,'TA12':0.7092612,'TC12':0.496298609,\n",
    "                  'TT12':-0.586873894,'GG13':-0.334563735,'GA14':0.76384993,'GC14':-0.53702517,\n",
    "                  'TG17':-0.798146133,'GG19':-0.66680873,'TC19':0.353183252,'CC20':0.748072092,\n",
    "                  'TG20':-0.367266772,'AC21':0.568209132,'CG21':0.329072074,'GA21':-0.836456755,\n",
    "                  'GG21':-0.782207584,'TC22':-1.029692957,'CG23':0.856197823,'CT23':-0.463207679,\n",
    "                  'AA24':-0.579492389,'AG24':0.649075537,'AG25':-0.077300704,'CG25':0.287935617,\n",
    "                  'TG25':-0.221637217,'GT27':0.117877577,'GG29':-0.697740024}\n",
    "    for i,nuc in enumerate(s_list):\n",
    "        key = nuc+str(i+1)\n",
    "        if sing_nuc_hash.has_key(key):\n",
    "            nuc_score = sing_nuc_hash[key]\n",
    "        else:\n",
    "            nuc_score = 0\n",
    "        #nuc_score = score_mat[i,nuc_hash[nuc]]\n",
    "        score = score+nuc_score\n",
    "        if i<29:\n",
    "            dinuc = nuc+s[i+1]+str(i+1)\n",
    "            if dinuc in dinuc_hash.keys():\n",
    "                score = score+dinuc_hash[dinuc]\n",
    "    partial_score = math.e**-score\n",
    "    final_score = 1/(1+partial_score)\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scoring function was then run on every long sgRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing as dp\n",
    "\n",
    "def run_doench(db_name, sql_version=\"MySQL\", firewall=False):\n",
    "    db_con = dp.DatabaseConnection(sql_version, db_name=db_name, firewall=firewall)\n",
    "    \n",
    "    rows = db_con.fetch_query(\"SELECT SgID, LongSg FROM SgRNATargetInformation\")\n",
    "    \n",
    "    doench_dict = {\"DoenchScore\": []}\n",
    "    sg_dict = {\"SgID\": [], \"LongSg\": []}\n",
    "    for row in rows:\n",
    "        if sql_version == \"MSSQL\":\n",
    "            sgID = row.SgID\n",
    "            longSg = row.LongSg\n",
    "        else:\n",
    "            sgID, longSg = row\n",
    "            longSg = str(longSg)\n",
    "        d_score = calc_doench_score(longSg)\n",
    "        doench_dict[\"DoenchScore\"] += [d_score]\n",
    "        sg_dict[\"SgID\"] += [sgID]\n",
    "        sg_dict[\"LongSg\"] += [longSg]\n",
    "    db_con.update_many_rows(doench_dict, sg_dict, \"SgRNATargetInformation\")\n",
    "    db_con.close_cursor()\n",
    "    db_con.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_doench(\"miR-test\", firewall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MaxDoenchScore\n",
    "\n",
    "Once the Doench score had been calculated for each extended sgRNA, the maximum Doench score for the sgRNA was determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing as dp\n",
    "\n",
    "def find_max_doench(db_name, sql_version=\"MySQL\", firewall=False):\n",
    "    db_con = dp.DatabaseConnection(sql_version, db_name=db_name, firewall=firewall)\n",
    "    rows = db_con.fetch_query(\"\"\"SELECT s.SgID, t.DoenchScore \n",
    "FROM SingleGuideRNA AS s \n",
    "JOIN SgRNATargetInformation AS t \n",
    "ON s.SgID = t.SgID\"\"\")\n",
    "    max_dict = {}\n",
    "    for row in rows:\n",
    "        if sql_version == \"MSSQL\":\n",
    "            sg = row.SgID\n",
    "            score = row.DoenchScore\n",
    "        else:\n",
    "            sg, score = row\n",
    "        if sg in max_dict:\n",
    "            if score > max_dict[sg]:\n",
    "                max_dict[sg] = score\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            max_dict[sg] = score\n",
    "    sg_dict = {\"SgID\": []}\n",
    "    do_dict = {\"MaxDoenchScore\": []}\n",
    "    for key, val in max_dict.iteritems():\n",
    "        sg_dict[\"SgID\"] += [key]\n",
    "        do_dict[\"MaxDoenchScore\"] += [val]\n",
    "        \n",
    "    db_con.update_many_rows(do_dict, sg_dict, \"SingleGuideRNA\")\n",
    "    db_con.close_cursor()\n",
    "    db_con.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_max_doench(\"miR-test\", firewall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azimuth Score\n",
    "\n",
    "This on-target scoring method was updated in 2015 (<a href=\"https://doi.org/10.1101/021568\">Preprint</a>)/2016 (<a href=\"https://doi.org/10.1038/nbt.3437\">Nature Biotechnology Paper</a>) with the Azimuth score. I contacted Microsoft and recieved an API key which enabled me to access the API and submit requests for the scores of my sgRNAs. The documentation for this API is <a href=\"https://studio.azureml.net/apihelp/workspaces/ee5485c1d9814b8d8c647a89db12d4df/webservices/72e5e606de0b4fa0bcde57666f0ddcba/endpoints/c24d128abfaf4832abf1e7ef45db4b54/score\">here</a>. The source code is now up at <a href=\"https://github.com/MicrosoftResearch/Azimuth\">GitHub</a> and there is a <a href=\"http://www.broadinstitute.org/rnai/public/analysis-tools/sgrna-design\">web interface</a> as well. \n",
    "\n",
    "### From flat file\n",
    "\n",
    "It seems they tweaked the scoring over time. The Azimuth scores from the API are now slightly different than the original and the API is no longer being supported. The older score will therefore be loaded from a flat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_processing as dp\n",
    "\n",
    "def import_azimuth_score(db_name, sql_version=\"MSSQL\", firewall=False):\n",
    "    \"\"\"\n",
    "        Fetches Azimuth score from flat file\n",
    "    \"\"\"\n",
    "    load_dict = {\"SgID\": [], \"PriID\": [], \"SgStart\": []}\n",
    "    az_dict = {\"AzimuthScore\": []}\n",
    "    \n",
    "    df = pd.read_csv(\"sgRNA Scoring/SgRNATargetInformation_table.csv\", header=0, index_col=0)\n",
    "    for sgID, row in df.iteritems():\n",
    "        load_dict[\"SgID\"] += [sgID]\n",
    "        load_dict[\"PriID\"] += [row[\"PriID\"]]\n",
    "        load_dict[\"SgStart\"] += [int(row[\"SgStart\"])]\n",
    "        az_dict[\"AzimuthScore\"] += [float(row[\"AzimuthScore\"])]\n",
    "    \n",
    "    db_con = dp.DatabaseConnection(sql_version, db_name=db_name, firewall=firewall)\n",
    "    db_con.update_many_rows(az_dict, load_dict, \"SgRNATargetInformation\")\n",
    "    db_con.close_cursor()\n",
    "    db_con.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_azimuth_score(\"miR-test\", firewall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Azimuth API\n",
    "\n",
    "The score can be fetched using the API and imported into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import data_processing as dp\n",
    "\n",
    "### Azimuth (updated Doench) Score ###\n",
    "\n",
    "def get_azimuth_score(api_key_file, db_name, sql_version=\"MySQL\", firewall=False):\n",
    "        \n",
    "    db_con = dp.DatabaseConnection(sql_version, db_name=db_name, firewall=firewall)\n",
    "    \n",
    "    # create new column\n",
    "    db_con.add_column(\"AzimuthScorev2\", \"FLOAT\", \"SgRNATargetInformation\")\n",
    "\n",
    "    STEP = 1000\n",
    "    \n",
    "    rows = db_con.fetch_query(\"SELECT SgID, PriID, SgStart, LongSg FROM SgRNATargetInformation\")\n",
    "    \n",
    "    sg_dict = {\"SgID\": [], \"LongSg\": [], \"PriID\": [], \"SgStart\": []}\n",
    "    az_dict = {\"AzimuthScorev2\": []}\n",
    "    for row in rows:\n",
    "        if sql_version == \"MSSQL\":\n",
    "            sg_dict[\"SgID\"] += [row.SgID]\n",
    "            sg_dict[\"LongSg\"] += [row.LongSg]\n",
    "            sg_dict[\"PriID\"] += [row.PriID]\n",
    "            sg_dict[\"SgStart\"] += [row.SgStart]\n",
    "        else:\n",
    "            sgID, priID, sgStart, longSg = row\n",
    "            sg_dict[\"SgID\"] += [sgID]\n",
    "            sg_dict[\"LongSg\"] += [str(longSg)]\n",
    "            sg_dict[\"PriID\"] += [str(priID)]\n",
    "            sg_dict[\"SgStart\"] += [sgStart]\n",
    "    \n",
    "    num_sg = len(sg_dict[\"LongSg\"])\n",
    "    num_chunks = int(math.ceil(num_sg/float(STEP)))\n",
    "    for n in range(num_chunks):\n",
    "        longSg_start = n*STEP\n",
    "        longSg_end = min(n*STEP+STEP, num_sg)\n",
    "        longSg_chunk = sg_dict[\"LongSg\"][longSg_start:longSg_end]\n",
    "        \n",
    "        sgRNAs = [[sg.upper(), -1, -1] for sg in longSg_chunk]\n",
    "\n",
    "        data = {\"Inputs\": {\n",
    "                            \"input1\":{\n",
    "                                      \"ColumnNames\": [\"sequence\", \"cutsite\", \"percentpeptide\"],\n",
    "                                      \"Values\": sgRNAs\n",
    "                                     },        \n",
    "                          },\n",
    "                \"GlobalParameters\": {}\n",
    "               }\n",
    "\n",
    "        body = str.encode(json.dumps(data))\n",
    "\n",
    "        url = 'https://ussouthcentral.services.azureml.net/workspaces/ee5485c1d9814b8d8c647a89db12d4df/services/c24d128abfaf4832abf1e7ef45db4b54/execute?api-version=2.0&details=true'\n",
    "        with open(api_key_file, \"r\") as api:\n",
    "            api_key = api.readline()\n",
    "        headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "        req = urllib2.Request(url, body, headers)\n",
    "\n",
    "        try:\n",
    "            response = urllib2.urlopen(req)\n",
    "\n",
    "            result = response.read()\n",
    "            parsed_results = json.loads(result)\n",
    "            listOfResults = parsed_results[\"Results\"][\"output2\"][\"value\"][\"Values\"]\n",
    "            for m in range(len(listOfResults)):\n",
    "                score = float(listOfResults[m][0])\n",
    "                az_dict[\"AzimuthScorev2\"] += [score]\n",
    "        except urllib2.HTTPError, error:\n",
    "            print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "            # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "            print(error.info())\n",
    "\n",
    "            print(json.loads(error.read()))\n",
    "        time.sleep(30)\n",
    "    db_con.update_many_rows(az_dict, sg_dict, \"SgRNATargetInformation\")\n",
    "    db_con.close_cursor()\n",
    "    db_con.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_azimuth_score(\"Azimuth_API_key.txt\", \"miR-test\", firewall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Azimuth Score\n",
    "\n",
    "The maximum azimuth score for all target sites for each sgRNA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing as dp\n",
    "\n",
    "def find_max_azimuth(db_name, sql_version=\"MySQL\", firewall=False):\n",
    "    db_con = dp.DatabaseConnection(sql_version, db_name=db_name, firewall=firewall)\n",
    "    \n",
    "    rows = db_con.fetch_query(\"\"\"SELECT s.SgID, t.AzimuthScore \n",
    "FROM SingleGuideRNA AS s \n",
    "JOIN SgRNATargetInformation AS t \n",
    "ON s.SgID = t.SgID\"\"\")\n",
    "    \n",
    "    max_dict = {}\n",
    "    for row in rows:\n",
    "        if sql_version == \"MSSQL\":\n",
    "            sg = row.SgID\n",
    "            score = row.AzimuthScore\n",
    "        else:\n",
    "            sg, score = row\n",
    "        if sg in max_dict:\n",
    "            if score > max_dict[sg]:\n",
    "                max_dict[sg] = score\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            max_dict[sg] = score\n",
    "    sg_dict = {\"SgID\": []}\n",
    "    do_dict = {\"MaxAzimuthScore\": []}\n",
    "    for key, val in max_dict.iteritems():\n",
    "        sg_dict[\"SgID\"] += [key]\n",
    "        do_dict[\"MaxAzimuthScore\"] += [val]\n",
    "        \n",
    "    db_con.update_many_rows(do_dict, sg_dict, \"SingleGuideRNA\")\n",
    "    db_con.close_cursor()\n",
    "    db_con.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_max_azimuth(\"miR-test\", firewall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sgRNA Scorer\n",
    "\n",
    "The Church lab also came up with an on-target activity scoring algorthm call <a href=\"https://crispr.med.harvard.edu/\">sgRNA Scorer</a>. <a href=\"http://www.nature.com/nmeth/journal/v12/n9/full/nmeth.3473.html\">This paper</a> describes how this method was developed. The input for the scoring is a fasta file with the sgRNA + PAM sequence. This information can be sliced from the longSg sequence using the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing as dp\n",
    "\n",
    "def get_sgRNAs(out_file, db_name, sql_version=\"MySQL\", firewall=False):\n",
    "    \"\"\"\n",
    "        Fetches the sgRNA and PAM sequences from the database for sgRNA Scorer\n",
    "    \"\"\"\n",
    "    db_con = dp.DatabaseConnection(sql_version, db_name=db_name, firewall=firewall)\n",
    "    rows = db_con.fetch_query(\"SELECT SgID, LongSg FROM SgRNATargetInformation\")\n",
    "    db_con.close_cursor()\n",
    "    db_con.close_connection()\n",
    "    \n",
    "    with open(out_file, \"w\") as fout:\n",
    "        for row in rows:\n",
    "            if sql_version == \"MSSQL\":\n",
    "                sgID = row.SgID\n",
    "                longSg = row.LongSg\n",
    "            else:\n",
    "                sgID, longSg = row\n",
    "                longSg = str(longSg)\n",
    "            header_str = \">{}_{}\\n\".format(sgID, longSg)\n",
    "            fout.write(header_str)\n",
    "            fout.write(\"{}\\n\".format(longSg[4:-3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sgRNAs(\"sgRNA Scoring/miR_sgRNAs.fa\", \"miR-test\", firewall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stand alone code can be downloaded from this <a href=\"https://crispr.med.harvard.edu/\">website</a> and the <a href=\"http://svmlight.joachims.org/\">svm-light binaries</a> are also necessary . The sgRNAs were then scored relative to the human exome sgRNAs included with the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"Downloaded Programs/sgRNA.Scorer.1.0\"\n",
    "!python scoreMySites.py \"../../sgRNA Scoring/miR_sgRNAs.fa\" Hg SP miR_sgRNAScorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for each sgRNA were then imported into the SingleGuideRNA table. For sgRNAs which had multiple possible PAM sites, the highest score is added to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing as dp\n",
    "\n",
    "def import_sgRNAScorer(in_file, db_name, sql_version=\"MySQL\", firewall=False):\n",
    "    \"\"\"\n",
    "        Imports sgRNA Scorer score\n",
    "    \"\"\"\n",
    "    score_dict = {}\n",
    "    with open(in_file, \"r\") as fin:\n",
    "        # skip header line\n",
    "        fin.next()\n",
    "        for line in fin:\n",
    "            ele = line.strip(\"\\n\").split(\"\\t\")\n",
    "            sgID = int(ele[0].split(\"_\")[0])\n",
    "            if sgID not in score_dict:\n",
    "                score_dict[sgID] = float(ele[2])\n",
    "            else:\n",
    "                if score_dict[sgID] < float(ele[2]):\n",
    "                    score_dict[sgID] = float(ele[2])\n",
    "                else:\n",
    "                    continue\n",
    "    sg_dict = {\"SgID\": []}\n",
    "    sg_scorer = {\"SgRNAScorer\": []}\n",
    "    for key, val in score_dict.iteritems():\n",
    "        sg_dict[\"SgID\"] += [key]\n",
    "        sg_scorer[\"SgRNAScorer\"] += [val]\n",
    "    db_con = dp.DatabaseConnection(sql_version, db_name=db_name, firewall=firewall)\n",
    "    db_con.update_many_rows(sg_scorer, sg_dict, \"SingleGuideRNA\")\n",
    "    db_con.close_cursor()\n",
    "    db_con.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_sgRNAScorer(\"Downloaded Programs/sgRNA.Scorer.1.0/miR_sgRNAScorer.FinalOutput.txt\",\n",
    "                   \"miR-test\", firewall=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
